\documentclass{article}

% Basic packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue, pdfborder={0 0 0}]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cite}

% Title and author
\title{A Proposal for the Use of Backtranslation to Decode Whale Songs}
\author{Morgan Rivers \\
        Department of Physics \\
        Freie Universität Berlin \\    
        \texttt{danielmorganrivers@gmail.com}
}

\begin{document}

\maketitle

\begin{abstract}
This paper suggests a novel method for decoding whale songs using backtranslation techniques. I propose a framework that involves training models to translate whale vocalizations to English and back, ensuring consistency and accuracy through reinforcement learning. The approach is to maximize the likelihood of joint probability distributions, providing a robust model for understanding animal communication.
\end{abstract}

\section{Introduction}
Decoding whale songs presents a unique challenge in the field of animal communication. I believe communication with whales will both teach us for the first time a complete language of another species, as well as enable communication with many more animals. This will be critical to both understand the desires and steward the environment for these animals, as well as raise popular awareness of how humanity is affecting wild animals on this planet.\\
In this paper, I propose an unsupervised translation approach motivated by understanding animal communication. This involves using backtranslation techniques to create a model that can translate whale vocalizations into English and back, ensuring that the translations are contextually and semantically accurate.\\ \\
I will use the term "Whale" to refer to the language Whales are speaking. For example, "she understands both Whale and English."

\section{Related Work}
Studies on neural networks, such as AlphaFold for molecular interactions, have shown the potential for applying similar techniques to decode whale songs. \\ \\
One paper specifically about whales is interesting, and partially motivated this work:
"A Theory of Unsupervised Translation Motivated by Understanding Animal Communication" (https://arxiv.org/abs/2211.11081) \\ \\
However, it lacks specificity and clarity for their favored approach, and tries to cover a lot of ground which is not entirely needed for simply trying to translate Whale.\\ \\
Several studies have explored backtranslation techniques to improve neural machine translation. Notably, the work by Lambert et al. provides a method for improving unsupervised neural machine translation with semantically weighted backtranslation, which is particularly relevant for morphologically rich and low-resource languages \cite{backtranslation1}. Another significant contribution is by the Language Resource Association, which presents an approach for training data self-correction to enhance unsupervised neural machine translation \cite{backtranslation2}.


\section{Introduction}
This paper proposes a novel approach to translate whale vocalizations into human language using transformer-based language models. The method involves training multiple models on whale vocalizations and human language, then using a series of translation steps and penalty functions to refine the translation process.

\section{Data Preparation}
The initial data consists of a large corpus of whale vocalizations, encoded as text with accompanying environmental context. Each vocalization is attributed to a specific whale, with information about the time and surrounding whales. This data is formatted similarly to a theatrical script, with each entry requiring less than 2048 tokens.

\section{Model Architecture}
Three decoder-only transformer language models are utilized:

\begin{itemize}
    \item $M_w$: Trained solely on whale vocalizations
    \item $M_e$: Trained solely on English text
    \item $M_{we}$: Trained on both whale vocalizations and English text
\end{itemize}

\section{Training Process}
\subsection{English Context Fine-tuning}
$M_e$ is fine-tuned on relevant English context, including:
\begin{itemize}
    \item English plays
    \item Scientific literature about whales
    \item Biology and science textbooks focusing on whales
    \item Specific context about the region and family of whales being studied
\end{itemize}

\subsection{Whale Language Model Training}
$M_w$ is trained to predict the next tokens in the whale text, excluding environmental context unless dependent on whale actions.

\subsection{Bilingual Model Training}
$M_{we}$ is trained on both whale vocalizations and English text, with a focus on maintaining proficiency in both languages.

\section{Translation Methodology}
\subsection{Whale to English Translation}
$M_{we}$ is prompted with:
\begin{verbatim}
[Environmental context]
[Paragraph of Whale]
Please translate the above paragraph to English:
\end{verbatim}

The resulting translation is denoted as \textit{whale\_to\_english}.

\subsection{English to Whale Back-translation}
The \textit{whale\_to\_english} translation is then used to prompt $M_{we}$ again:
\begin{verbatim}
[Environmental context]
[Paragraph of Whale translated to English (whale_to_english)]
Please translate the above paragraph to Whale:
\end{verbatim}

The result is denoted as \textit{whale\_to\_english\_back\_to\_whale}.

\section{Penalty Functions}
Three penalty functions are employed to refine the translation process:

\begin{itemize}
    \item $P_{we}$: English likelihood penalty
    \item $P_{wew}$: Whale context probability penalty
    \item $P_{sd}$: Semantic distance penalty
\end{itemize}

\subsection{English Likelihood Penalty ($P_{we}$)}
$P_{we}$ is calculated using $M_e$ to evaluate the average probability per token of the \textit{whale\_to\_english} paragraph. A high $P_{we}$ indicates an unlikely English paragraph.

\subsection{Whale Context Probability Penalty ($P_{wew}$)}
$P_{wew}$ is determined by inserting \textit{whale\_to\_english\_back\_to\_whale} into the overall whale text and calculating the probability using $M_w$. A high $P_{wew}$ indicates low probability of the translated whale text appearing in that context.

\subsection{Semantic Distance Penalty ($P_{sd}$)}
$P_{sd}$ measures the semantic distance between the original whale paragraph and \textit{whale\_to\_english\_back\_to\_whale}. A high $P_{sd}$ indicates significant divergence in meaning. This penalty is weighted higher than $P_{wew}$ to prioritize meaning preservation.

\section{Putting it all together}
The penalties are used for reinforcement learning applied to $M_{we}$. This process is repeated for all available paragraphs, potentially tens of thousands of times, effectively fine-tuning the model on a paragraph-by-paragraph basis.
Here is a summary of the training algorithm above:
\begin{algorithm}
\caption{Backtranslation Algorithm}
\begin{algorithmic}[1]
\STATE Initialize parameters $\theta$
\FOR{each iteration}
\STATE Translate whale vocalizations to English
\STATE Calculate probability penalty $Pwe$
\STATE Translate English back to whale
\STATE Calculate contextual and semantic penalties $Pwew$ and $Psd$
\STATE Update parameters $\theta$ based on combined penalties
\ENDFOR
\end{algorithmic}
\end{algorithm}
\subsection{Datasets}
The datasets provided a comprehensive set of whale vocalizations and environmental contexts, essential for training and evaluating our models.\\

I would primarily be interested in using two whale datasets for this work: 

1) Seven months of recordings from the THEMO observatory \cite{themo}, containing both ambient and anthropogenic noise but no whale clicks, 

2) Recordings of 1203 tagged clicks of a single whale from the Bahamas with 120 sec of noise-only recording and recordings of over 15,000 manually tagged clicks from multiple whales recorded on the Dominica Island together with 3.6 hours of noise-only data \cite{ceti}. \\ \\
Also see:
\begin{itemize}
    \item \href{https://www.projectceti.org/}{Project CETI}
    \item \href{https://www.projectceti.org/news-research-insights#publications}{Project CETI Publications}
    \item \href{https://www.youtube.com/playlist?list=PL-7Ju3RM0_oUKDIfBWlsOhONsXimWfMlF}{Project CETI Workshop - Decoding Communication in Nonhuman June 12-13, 2023 @ Simons Institute for the Theory of Computing}
\end{itemize}

For more detailed whale datasets, we will use recordings from socially segregated, sympatric sperm whale clans in the Atlantic Ocean \cite{rsos}. These recordings were conducted over a decade, from 2005 through 2015, covering approximately 2000 km\textsuperscript{2} along the western coast of Dominica (15.30° N, 61.40° W). Membership in social units was designated based on photo-identification analysis and long-term spatio-temporal coordination among unit members. Acoustic recordings were made during deep foraging dives and socializing at the surface, using various systems with flat frequency responses across ranges of at least 2–20 kHz and sampling rates of 44.1 kHz or higher. For this study, the focus will be on the EC1 clan, which predominates in the study area and neighboring waters \cite{rsos}.

\section{Validation}
Validation is performed using unseen whale vocalizations, comparing the accuracy of \textit{whale\_to\_english\_back\_to\_whale} to that of the training data.

\section{Potential Challenges}
\begin{enumerate}
    \item Poor performance of $M_w$
    \item Insufficient context for accurate translation
    \item Tendency towards common whale utterances in back-translation
    \item Development of a system to "cheat" the translation process
    \item Unclear direction of penalties for efficient learning
    \item Overfitting to specific training data
    \item Risk of translating only sounds and grammar without capturing meaning
\end{enumerate}
\section{Test Case Refinement - Low resource human languages}
There is a need to refine the methodology with a test case with a known solution. This will:
\begin{enumerate}
    \item Identify the appropriate size of the language model needed.
    \item Help to experiment and improve methodology for the test case, which should also improve the whale case.
\end{enumerate}

It is important to use a language that has very little leakage into the training set for an English language model, making it reasonably analogous to the translation from whale.

The methodology to test and refine the method outlined above with low resource human languages is as follows.

Using an isolate language ensures there is no leakage of that language's grammar or words into English, preventing an easier translation due to prior knowledge.

The context of the recordings should be significantly different from the context in which English is spoken. This similarity to the different context of whale vocalizations makes the task more analogous.

Starting with a spoken language and using a similar approach as the SETI project is also beneficial. This includes grouping into phonemes and converting those phonemes into letters of the English alphabet.

The use of a language that is particularly complex and foreign to English speakers is more appropriate. The more complex and alien the language, the more analogous it is expected to be to whale vocalizations.

Additionally, certain features of whale communication can be incorporated into test languages. For example, human languages with clicks and tones may be useful due to whales' use of clicks and tone as markers of meaning.

\subsection{Selection of Test Language}
In selecting a test language, several criteria were considered to ensure the chosen language is appropriately analogous to whale vocalizations:
\begin{enumerate}
    \item Linguistic Isolation: The language should be a linguistic isolate or significantly different from English in terms of structure and origins.
    \item Extensive Documentation: There should be a substantial corpus of digitized texts or recordings available for study.
    \item Cultural Distinction: The culture associated with the language should be distinct and relatively unrelated to Western cultures to avoid cultural leakage.
    \item Availability of Audio Recordings: Ideally, the language should have historical audio recordings that capture it in a native, traditional context.
\end{enumerate}

Several languages were considered based on these criteria:

\paragraph{Old Javanese (Kawi)} An ancient language used in Java from the 8th to 16th centuries, with a rich literary tradition and a distinct Hindu-Buddhist culture. Significant corpus of digitized texts is available, including epic poems and religious texts.

\paragraph{Classical Nahuatl} The language of the Aztec Empire, part of the Uto-Aztecan family, with extensive digitized texts including the Florentine Codex and Cantares Mexicanos. It offers a unique view of Mesoamerican culture.

\paragraph{Navajo} An Athabaskan language with a complex structure and extensive historical recordings, including those made during World War II by Navajo Code Talkers. It represents a distinct cultural heritage with minimal leakage into mainstream English culture.

\paragraph{Ket} A Yeniseian language spoken in Siberia, with a limited number of speakers and some recorded materials. It is an isolate with unique linguistic features and minimal exposure to Western culture.

\paragraph{Xhosa and Zulu} Bantu languages with click consonants, spoken in South Africa. They have significant recordings and a rich cultural heritage, though there has been some cultural leakage into Western consciousness. \\ \\
After careful consideration, \textbf{Navajo} was selected as the most suitable test language. Its polysynthetic structure, extensive historical recordings, and minimal cultural leakage make it an ideal candidate for refining the translation methodology. Additionally, the availability of recordings in traditional contexts provides valuable data for testing the translation framework.


\subsection{Potential Datasets for Test Task}
\begin{itemize}
    \item The Endangered Languages Archive (ELAR)
    \item The World Oral Literature Project, which has collected recordings of endangered languages worldwide
\end{itemize}

These datasets allow testing when converting from a given spoken language to a series of letters corresponding to vocals, similar to Navajo. It helps establish a baseline for the ability of an unsupervised translation algorithm to effectively translate an unknown language into a known language.

\section{Miscellaneous end notes}

It might be better to have an ensemble of answers for a given translation whale\_to\_english\_back\_to\_whale, and then boost the model with the best of these answers. That addresses 5, as it gives the model a direction to move the prediction.

Note: this ends up looking like an auto encoder! We are expanding into English language token space, and then going back to the original space.


Another way to help it start in the right direction is to use statistical matching: most common concepts are assumed to be all the whale words in English. So then you can give it context: given the above best guess translation, improve it to a better one. And then you pick the best answer closest to the input.

\section{Conclusion}
This approach presents a novel method for translating whale vocalizations to human language using transformer-based models and reinforcement learning. While challenges exist, the use of multiple models and carefully designed penalty functions offers a promising direction for future research in animal communication translation.

\begin{thebibliography}{9}

\bibitem{themo}
Roee Diamant, Anthony Knapr, Shlomo Dahan, Ilan Mardix, John Walpert, and Steve DiMarco. 
\textit{Themo: The texas a\&m-university of haifa-eastern mediterranean observatory}. 
In 2018 OCEANS-MTS/IEEE Kobe Techno-Oceans (OTO), pages 1–5. IEEE, 2018.

\bibitem{ceti}
Link to implementation code and database [online]. available at: 
\url{https://drive.google.com/drive/folders/1HkGZcpYbrft3pGtqdt45bUZXVcrJVrdm?usp=sharing}.

\bibitem{rsos}
Luca M. Lambert, Hal Whitehead, Shane Gero, "Socially segregated, sympatric sperm whale clans in the Atlantic Ocean," \textit{Royal Society Open Science}, 2016, available at \url{http://dx.doi.org/10.1098/rsos.160061}.

\bibitem{backtranslation1}
Lambert et al., "Improved Unsupervised Neural Machine Translation with Semantically Weighted Back Translation for Morphologically Rich and Low Resource Languages," \textit{ACL Anthology}, 2022, available at \url{https://aclanthology.org/2022.acl-long.2}.

\bibitem{backtranslation2}
Language Resource Association, "Improving Unsupervised Neural Machine Translation via Training Data Self-Correction," \textit{LREC-COLING 2024}, pages 8942–8954, May 2024, available at \url{https://aclanthology.org/2024.lrec-coling.8942}.


\end{thebibliography}
\end{document}
